{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0162d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e152dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate function\n",
    "def score(pred,label):\n",
    "    plus=np.sum(pred*label,axis=1)\n",
    "    minus=np.sum(pred*(1-label),axis=1)\n",
    "    return np.mean((plus-minus)/np.sum(label,axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbfc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  file path\n",
    "data_path = r'.\\all_file'\n",
    "df_test_label = pd.read_csv(os.path.join(data_path,'test_label.csv'),index_col='ID') ##  load label\n",
    "df_submit = pd.read_csv(os.path.join(data_path,'submit.csv'),index_col='ID') ## load submit template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ffc7b",
   "metadata": {},
   "source": [
    "# classfier for rootcause1 by text-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31cbd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "features = ['feature0', 'feature1', 'feature2', 'feature11', 'feature12','feature13', 'feature15',\n",
    "        'feature16', 'feature17','feature18',\n",
    "        'feature19',\n",
    "        'feature28_0', 'feature28_1', 'feature28_2', 'feature28_3',\n",
    "        'feature28_4', 'feature28_5', 'feature28_6', 'feature28_7',\n",
    "        'feature36_0', 'feature36_1', 'feature36_2', 'feature36_3',\n",
    "        'feature36_4', 'feature36_5', 'feature36_6', 'feature36_7', 'feature60',\n",
    "        'feature61_0', 'feature61_1', 'feature61_2', 'feature61_3',\n",
    "        'feature61_4', 'feature61_5', 'feature61_6', 'feature61_7','feature_edge','feature_distance','length']\n",
    "\n",
    "train_data_process = 'train_for_textcnn'\n",
    "test_data_process = 'test_for_textcnn'\n",
    "test_data_raw = 'test'\n",
    "\n",
    "files = os.listdir(os.path.join(data_path,test_data_process))\n",
    "files.sort(key=lambda x:int(x[:-4]))\n",
    "all_feature = []\n",
    "for filename in files:\n",
    "    df = pd.read_csv(os.path.join(data_path,test_data_process,filename),index_col = 0)\n",
    "    list_tmp = []\n",
    "    for nd in features:\n",
    "        for i in df[nd].values:\n",
    "            if type(i) == str:\n",
    "                if len(i.split(';'))> 1:\n",
    "                    i = np.array(i.split(';')).astype(float).mean()\n",
    "            list_tmp.append(i)\n",
    "    all_feature.append(list_tmp)\n",
    "all_feature = np.array(all_feature).reshape(-1,len(features),30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccaecbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "from keras.models import load_model\n",
    "model_name = 'textcnn_with_attention_root1_old.h5'\n",
    "model = load_model(os.path.join(data_path,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dc4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict result for mean\n",
    "result_new = []\n",
    "res = model.predict(all_feature)\n",
    "for i in res:\n",
    "    if i[0]>0.8:\n",
    "        result_new.append(0)\n",
    "    else:\n",
    "        result_new.append(1)\n",
    "\n",
    "## predict result for each row \n",
    "pre_result = pd.DataFrame(columns=['index','length','pre=1','pre=0','score'])\n",
    "for m in range(600):\n",
    "    test_array = []\n",
    "    raw = pd.read_csv(os.path.join(data_path,test_data_process,'{}.csv'.format(m)) ,index_col=0)\n",
    "    raw = raw[features]\n",
    "    for i in range(len(raw)):\n",
    "        line = raw.loc[i,:].to_frame().T\n",
    "        line_30 = pd.DataFrame(np.repeat(line.values,30,axis=0))\n",
    "        line_30.columns = line.columns\n",
    "        test_array.append(line_30.T.values)\n",
    "    test_array = np.array(test_array)\n",
    "    res2 = model.predict(test_array)\n",
    "    result2 = np.argmax(res2,axis=1)\n",
    "    t = []\n",
    "    t.append(m)\n",
    "    t.append(len(result2))\n",
    "    t.append(result2.sum())\n",
    "    t.append(len(result2)-result2.sum())\n",
    "    t.append(res2[:,1].sum()/len(result2))\n",
    "    pre_result.loc[m,:]=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe4a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  key feature fluctuate\n",
    "feature13_std = []\n",
    "feature15_std = []\n",
    "files = os.listdir(os.path.join(data_path,test_data_raw))\n",
    "files.sort(key=lambda x:int(x[:-4]))\n",
    "for filename in files:\n",
    "    df = pd.read_csv(os.path.join(data_path,test_data_raw,filename),index_col = 0)\n",
    "    feature13_std.append(df['feature13'].std())\n",
    "    feature15_std.append(df['feature15'].std())\n",
    "feature13_std = np.array(feature13_std)\n",
    "feature15_std = np.array(feature15_std)\n",
    "feature13_std[np.isnan(feature13_std)] = 0\n",
    "feature15_std[np.isnan(feature15_std)] = 0\n",
    "feature13_std = feature13_std/np.max(feature13_std)\n",
    "feature15_std = feature15_std/np.max(feature15_std)\n",
    "feature_fil = feature13_std+feature15_std\n",
    "\n",
    "add_1 = []\n",
    "for i in range(len(pre_result['score'])):\n",
    "    if pre_result['score'][i] == 1:\n",
    "        add_1.append(i)\n",
    "for val in np.where(np.array(feature_fil)>0.6)[0]:  ## 用于筛选特征13和15波动性比较大的样本\n",
    "    if pre_result.loc[val,'score']>0.14:\n",
    "        add_1.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e33c452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit['Root1'] = result_new\n",
    "df_submit.loc[add_1,['Root1']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618aa5c0",
   "metadata": {},
   "source": [
    "# clssfier for rootcause2 & rootcause3 by ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36e266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "feature_select =  ['feature0', 'feature1', 'feature2',\n",
    "       'feature11', 'feature12','feature13', 'feature14', 'feature15', \n",
    "       'feature16','feature17','feature18', 'feature19', 'feature60',\n",
    "       'feature20_distance','featureY_if','featureX_if','featureY_mean','featureX_mean'] \n",
    "\n",
    "df_train = pd.read_csv(os.path.join(data_path,'train_for_ml.csv'),index_col = 0)\n",
    "df_test = pd.read_csv(os.path.join(data_path,'test_for_ml.csv'),index_col = 0)\n",
    "\n",
    "df_train_label = pd.read_csv(os.path.join(data_path,'train_label.csv'),index_col='sample_index')\n",
    "df_train_label['Root1'] = df_train_label['root-cause(s)'].apply(lambda x : 1 if 'rootcause1' in x else 0 )\n",
    "df_train_label['Root2'] = df_train_label['root-cause(s)'].apply(lambda x : 1 if 'rootcause2' in x else 0 )\n",
    "df_train_label['Root3'] = df_train_label['root-cause(s)'].apply(lambda x : 1 if 'rootcause3' in x else 0 )\n",
    "\n",
    "data_train = df_train.groupby('sample_index').mean()\n",
    "data_train['Root2'] = df_train_label.loc[:,'Root2']\n",
    "data_train['Root3'] = df_train_label.loc[:,'Root3']\n",
    "data_train.reset_index(inplace=True)\n",
    "data_train.dropna(inplace=True)\n",
    "\n",
    "data_test = df_test.groupby('sample_index').mean()\n",
    "data_test['Root2'] = df_test_label.loc[:,'Root2'] \n",
    "data_test['Root3'] = df_test_label.loc[:,'Root3'] \n",
    "data_test.reset_index(inplace=True)\n",
    "data_test.fillna(data_test.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d67625",
   "metadata": {},
   "source": [
    "## classfier for rootcause2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2670e622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1297\n",
      "1      65\n",
      "Name: Root2, dtype: int64\n",
      "(1362, 18) (1362,)\n",
      "0    484\n",
      "1    116\n",
      "Name: Root2, dtype: int64\n",
      "(600, 18) (600,)\n"
     ]
    }
   ],
   "source": [
    "X_train = data_train.loc[:,feature_select]\n",
    "y_train = data_train['Root2'] \n",
    "print(y_train.value_counts())\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "X_test = data_test.loc[:,feature_select]\n",
    "y_test = data_test['Root2']\n",
    "print(y_test.value_counts())\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0b8bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data standscaler\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "X = pd.concat([X_train,X_test])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab55dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1297), (1, 65)]\n",
      "[(0, 1216), (1, 1297)]\n"
     ]
    }
   ],
   "source": [
    "## data augmentation\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y_train).items()))\n",
    "X_train, y_train = SMOTEENN(random_state=0).fit_resample(X_train, y_train)\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "748b83f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model finished\n",
      "test model finished\n"
     ]
    }
   ],
   "source": [
    "## classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf_list = [SVC(),RandomForestClassifier(),MLPClassifier(max_iter=800),GradientBoostingClassifier(),AdaBoostClassifier()]\n",
    "clf = clf_list[-1]\n",
    "clf.fit(X_train, y_train)\n",
    "print('train model finished')\n",
    "\n",
    "y_predict_list = {}     \n",
    "threshold_1 = 7\n",
    "threshold_2 = 0\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    for sample_index,df in df_test.groupby('sample_index'):\n",
    "        df.fillna(df.interpolate(),inplace=True)\n",
    "        df.fillna(data_test.mean(),inplace=True)\n",
    "        if df['feature19'].std() > threshold_1 : # key feature fluctuate\n",
    "            X_test_one = df.loc[:,feature_select]\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)\n",
    "            if np.sum(y_predict_one) > threshold_2 :\n",
    "                y_predict_list[sample_index] = 1\n",
    "            else:\n",
    "                y_predict_list[sample_index] = 0\n",
    "        else:\n",
    "            X_test_one =data_test.loc[data_test['sample_index']==sample_index,feature_select]\n",
    "            X_test_one = X_test_one.values.reshape(1,-1)\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)[0]\n",
    "            y_predict_list[sample_index] = y_predict_one\n",
    "        # Too many missing values in some files\n",
    "        if sample_index in [142, 152, 153, 284, 305, 355, 361, 373, 420]:\n",
    "            y_predict_list[sample_index] = 1\n",
    "print('test model finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1033105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit['Root2'] = y_predict_list.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e282ef3",
   "metadata": {},
   "source": [
    "## classfier for rootcause3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3cac41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1251\n",
      "0     111\n",
      "Name: Root3, dtype: int64\n",
      "(1362, 18) (1362,)\n",
      "1    376\n",
      "0    224\n",
      "Name: Root3, dtype: int64\n",
      "(600, 18) (600,)\n"
     ]
    }
   ],
   "source": [
    "X_train = data_train.loc[:,feature_select]\n",
    "y_train = data_train['Root3'] \n",
    "print(y_train.value_counts())\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "X_test = data_test.loc[:,feature_select]\n",
    "y_test = data_test['Root3']\n",
    "print(y_test.value_counts())\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092a9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data standscaler\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "X = pd.concat([X_train,X_test])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc690165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 111), (1, 1251)]\n",
      "[(0, 1248), (1, 1220)]\n"
     ]
    }
   ],
   "source": [
    "## data augmentation\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y_train).items()))\n",
    "X_train, y_train = SMOTEENN(random_state=0).fit_resample(X_train, y_train)\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1476f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model finished\n",
      "test model finished\n"
     ]
    }
   ],
   "source": [
    "## test classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf_list = [SVC(),RandomForestClassifier(),MLPClassifier(max_iter=800),GradientBoostingClassifier(),AdaBoostClassifier()]\n",
    "clf = clf_list[0]\n",
    "clf.fit(X_train, y_train)\n",
    "print('train model finished')\n",
    "\n",
    "y_predict_list = {}     \n",
    "threshold_1 = 18000\n",
    "threshold_2 = 0\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    for sample_index,df in df_test.groupby('sample_index'):\n",
    "        df.fillna(df.interpolate(),inplace=True)\n",
    "        df.fillna(data_test.mean(),inplace=True)\n",
    "        if df['feature13'].std()> threshold_1 : # key feature fluctuate\n",
    "            X_test_one = df.loc[:,feature_select]\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)\n",
    "            if np.sum(y_predict_one) > threshold_2 :\n",
    "                y_predict_list[sample_index] = 1\n",
    "            else:\n",
    "                y_predict_list[sample_index] = 0\n",
    "        else:\n",
    "            X_test_one =data_test.loc[data_test['sample_index']==sample_index,feature_select]\n",
    "            X_test_one = X_test_one.values.reshape(1,-1)\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)[0]\n",
    "            y_predict_list[sample_index] = y_predict_one\n",
    "        # Too many missing values in some files \n",
    "        if sample_index in [142, 152, 153, 284, 305, 355, 361, 373, 420]:\n",
    "            y_predict_list[sample_index] = 1\n",
    "print('test model finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3b181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit['Root3'] = y_predict_list.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4a297",
   "metadata": {},
   "source": [
    "# Final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b553fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9191666666666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(df_submit.values,df_test_label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-py38_aiops]",
   "language": "python",
   "name": "conda-env-Anaconda3-py38_aiops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
