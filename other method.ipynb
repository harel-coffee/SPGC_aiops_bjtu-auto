{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa6043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "test_path = r'..\\test150'\n",
    "test_path_2 = r'..\\test600'\n",
    "train_path = r'..\\train'\n",
    "\n",
    "label_path = r'..'\n",
    "train_label_file = 'train_label.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91089c",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ce46be",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# feature process\n",
    "def feature60_process(s):\n",
    "    if isinstance(s,str):\n",
    "        s = eval(s.replace(';',','))\n",
    "        s = round(np.mean(s), 2)\n",
    "    return s\n",
    "\n",
    "def feature20_edge_count(feature20_series):\n",
    "    edge_list = [0,1,6,7,8,9,14,15,16,17,22,23,24,25,30,31]\n",
    "    feature20_edge_count = 0\n",
    "    for num in feature20_series:\n",
    "        if not np.isnan(num) and num in edge_list:\n",
    "            feature20_edge_count +=1\n",
    "    if feature20_series.isnull().all() : feature20_edge_count = np.nan\n",
    "    return feature20_edge_count\n",
    "    \n",
    "def feature20_distance_count(feature20_series):\n",
    "    feature20_series.dropna(inplace=True)\n",
    "    coordinate = []\n",
    "    for m in feature20_series:\n",
    "        coordinate.append((m-8*(m//8), m//8))\n",
    "    distance = 0\n",
    "    for a in coordinate:\n",
    "        for b in coordinate:\n",
    "            distance = distance + ((a[0]-b[0])**2 + (a[1]-b[1])**2)**0.5\n",
    "    return distance \n",
    "\n",
    "def featureXY_process(featureXY_series):\n",
    "    featureY_list = ['feature28','feature36','feature44','feature52']\n",
    "    featureX_list = ['feature61','feature69','feature77','feature85']\n",
    "    featureX = pd.DataFrame()\n",
    "    featureY = pd.DataFrame()\n",
    "    for featureX_name in featureX_list:\n",
    "        featureX = featureX.append(featureXY_series.loc[featureXY_series.index.str.contains(featureX_name)])\n",
    "    for featureY_name in featureY_list:\n",
    "        featureY = featureY.append(featureXY_series.loc[featureXY_series.index.str.contains(featureY_name)])\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        if len(featureX) == 0:\n",
    "            featureX_mean = np.nan\n",
    "            featureX_min = np.nan\n",
    "        else:\n",
    "            featureX_mean =np.nanmean(featureX)\n",
    "            featureX_min =np.nanmin(featureX)       \n",
    "        if len(featureY) == 0:\n",
    "            featureY_mean = np.nan\n",
    "            featureY_min = np.nan\n",
    "        else:\n",
    "            featureY_mean =np.nanmean(featureY)\n",
    "            featureY_min =np.nanmin(featureY)\n",
    "    return [featureY_mean, featureX_mean,featureY_min,featureX_min]\n",
    "\n",
    "def feature20XY_interference(feature20XY_series):\n",
    "    #返回 feature X/Y _if 两个个值 ， apply函数需要设置result_type = 'expand'\n",
    "    \n",
    "    feature_if_dict = {} # X和Y分别的信号值-干扰值\n",
    "    feature20 = feature20XY_series.loc[feature20XY_series.index.str.contains('feature20')]\n",
    "    featureY_list = ['feature28','feature36','feature44','feature62']\n",
    "    featureX_list = ['feature61','feature69','feature81','feature85']\n",
    "    \n",
    "    for featureV_list in [featureY_list,featureX_list]:\n",
    "        for featureV_name in featureV_list:\n",
    "            featureV = feature20XY_series.loc[feature20XY_series.index.str.contains(featureV_name)]\n",
    "            # 计算得到[[横坐标，纵坐标，V值] *8]\n",
    "            c_v_list = []\n",
    "            for i,value in zip(feature20,featureV):\n",
    "                if not (np.isnan(i) and np.isnan(value)):\n",
    "                        c_v = [i-8*(i//8), i//8, value]\n",
    "                        c_v_list.append(c_v)\n",
    "            interference = 0 #干扰计算公式，Σ(S -Σsn/dn)\n",
    "            for item in c_v_list:\n",
    "                for other_item in c_v_list:\n",
    "                    if other_item != item: \n",
    "                        distance =((item[0]-other_item[0])**2 + (item[1]-other_item[1])**2)**0.5\n",
    "                        if distance == 0: distance = 1 \n",
    "                        interference += other_item[2] / distance\n",
    "            if len(c_v_list) == 0 :\n",
    "                interference = np.nan \n",
    "            else:\n",
    "                interference = interference/len(c_v_list)\n",
    "                interference = round(interference,2)\n",
    "            feature_if_dict[featureV_name] = interference\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        featureY_if = np.nanmean([feature_if_dict[k] for k in featureY_list])\n",
    "        featureX_if = np.nanmean([feature_if_dict[k] for k in featureX_list])\n",
    "    return [featureY_if,featureX_if]\n",
    "\n",
    "    \n",
    "# def feature17_diff(feature17_series):\n",
    "#     return feature17_series.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc4520b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#  label file\n",
    "df_train_label = pd.read_csv(os.path.join(label_path,train_label_file),index_col='sample_index')\n",
    "train_label_dict = df_train_label['root-cause(s)'].to_dict()\n",
    "\n",
    "unknown_label_dict = {}\n",
    "for i in range(2984):\n",
    "    if i not in train_label_dict.keys():\n",
    "        unknown_label_dict[i] = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac77964a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# select feature\n",
    "feature_all = ['feature0', 'feature1', 'feature2', \n",
    "       'feature3_1','feature3_2', 'feature3_3', 'feature3_4', \n",
    "       'feature11', 'feature12', 'feature13','feature14', 'feature15', \n",
    "       'feature16', 'feature17', 'feature18','feature19', 'feature60',\n",
    "       'feature20_distance','feature20_edge',\n",
    "       'featureY_if','featureX_if','featureY_mean','featureX_mean',\n",
    "       'featureY_min','featureX_min'\n",
    "       ]\n",
    "feature_select =  ['feature0', 'feature1', 'feature2',\n",
    "       'feature11', 'feature12','feature13', 'feature14', 'feature15', \n",
    "       'feature16','feature17','feature18', 'feature19', 'feature60',\n",
    "       'feature20_distance','featureY_if','featureX_if','featureY_mean','featureX_mean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531b677",
   "metadata": {
    "code_folding": [
     1,
     20
    ]
   },
   "outputs": [],
   "source": [
    "# preprocess datatest + feature enginering\n",
    "def load_dataset_for_train(data_path,label_dict): \n",
    "    df_data = pd.DataFrame()\n",
    "    row_num = 0\n",
    "    for sample_index,cause in label_dict.items():\n",
    "        df = pd.read_csv(os.path.join(data_path,str(sample_index)+'.csv'))\n",
    "        row_num += len(df)    \n",
    "        df['feature60'] = df['feature60'].apply(feature60_process)\n",
    "        df['feature20_edge'] = df.loc[:,df.columns.str.contains('feature20')].apply(feature20_edge_count,axis=1)\n",
    "        df['feature20_distance'] = df.loc[:,df.columns.str.contains('feature20')].apply(feature20_distance_count,axis=1)\n",
    "        df[['featureY_if','featureX_if']] = df.apply(feature20XY_interference,axis=1,result_type='expand')\n",
    "        df[['featureY_mean','featureX_mean','featureY_min','featureX_min']] = df.apply(featureXY_process,axis=1,result_type='expand')\n",
    "        tmp = df.loc[:,feature_all]\n",
    "        tmp.insert(0,'causes_type',cause) \n",
    "        tmp.insert(1,'sample_index',sample_index)\n",
    "        df_data = df_data.append(tmp,ignore_index=True)\n",
    "    df_data.dropna(axis=1, inplace=True,how='all') #扔掉所有na的列\n",
    "    print('验证数据集维度',df_data.shape, row_num)\n",
    "    return df_data\n",
    "\n",
    "def load_dataset_for_test(data_path,file_num): \n",
    "    df_data = pd.DataFrame()\n",
    "    row_num = 0\n",
    "    for i in range(file_num):\n",
    "        df = pd.read_csv(os.path.join(data_path,str(i)+'.csv'))\n",
    "        row_num += len(df)    \n",
    "        df['feature60'] = df['feature60'].apply(feature60_process)\n",
    "        df['feature20_edge'] = df.loc[:,df.columns.str.contains('feature20')].apply(feature20_edge_count,axis=1)\n",
    "        df['feature20_distance'] = df.loc[:,df.columns.str.contains('feature20')].apply(feature20_distance_count,axis=1)\n",
    "        df[['featureY_if','featureX_if']] = df.apply(feature20XY_interference,axis=1,result_type='expand')\n",
    "        df[['featureY_mean','featureX_mean','featureY_min','featureX_min']] = df.apply(featureXY_process,axis=1,result_type='expand')\n",
    "        tmp = df.loc[:,feature_all]\n",
    "        tmp.insert(1,'sample_index',i)\n",
    "        df_data = df_data.append(tmp,ignore_index=True)\n",
    "    df_data.dropna(axis=1, inplace=True,how='all') #扔掉所有na的列\n",
    "    print('验证数据集维度',df_data.shape, row_num)\n",
    "    return df_data\n",
    "\n",
    "# df_unknown = load_dataset_for_train(train_path,unknown_label_dict)\n",
    "df_train = load_dataset_for_train(train_path,train_label_dict)\n",
    "df_val = load_dataset_for_test(test_path,150)\n",
    "df_test = load_dataset_for_test(test_path_2,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65659227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed data\n",
    "\n",
    "# df_train.to_csv('train.csv')\n",
    "# df_val.to_csv('test.csv')\n",
    "# df_test.to_csv('test_2.csv')\n",
    "\n",
    "df_train = pd.read_csv('train.csv',index_col = 0)\n",
    "df_val = pd.read_csv('test.csv',index_col = 0)\n",
    "df_test = pd.read_csv('test_2.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6c227",
   "metadata": {},
   "source": [
    "## binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66301ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select causes type\n",
    "\n",
    "# cause_type = 'cause1'\n",
    "cause_type = 'cause2'\n",
    "# cause_type = 'cause3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de981ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = df_train.groupby('sample_index').quantile(q=0.15)\n",
    "data_train = df_train.groupby('sample_index').mean()\n",
    "data_train.reset_index(inplace=True)\n",
    "data_train['causes_type'] = data_train['sample_index'].apply(lambda x:train_label_dict[x])\n",
    "data_train['label'] = data_train['causes_type'].apply(lambda x: 1 if cause_type in x else 0)\n",
    "data_train.dropna(inplace=True)\n",
    "\n",
    "# data_test = df_test_2.groupby('sample_index').quantile(q=0.5)\n",
    "data_test = df_test.groupby('sample_index').mean()\n",
    "data_test.reset_index(inplace=True)\n",
    "data_test.fillna(data_test.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043d9483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1362, 18) (1362,)\n",
      "(600, 18)\n"
     ]
    }
   ],
   "source": [
    "X_train = data_train.loc[:,feature_select]\n",
    "y_train = data_train['label']\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "X_test = data_test.loc[:,feature_select]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9137e745",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data standscaler\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer\n",
    "X = pd.concat([X_train,X_test])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88a3ecd",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1297), (1, 65)]\n",
      "[(0, 1216), (1, 1297)]\n"
     ]
    }
   ],
   "source": [
    "# data augmentation\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE,ADASYN,BorderlineSMOTE,KMeansSMOTE,SVMSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "print(sorted(Counter(y_train).items()))\n",
    "# X_train, y_train = RandomOverSampler(random_state=0).fit_resample(X_train, y_train)\n",
    "# X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_train, y_train)\n",
    "# X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "# X_train, y_train = ADASYN().fit_resample(X_train, y_train)\n",
    "# X_train, y_train = BorderlineSMOTE().fit_resample(X_train, y_train)\n",
    "# X_train, y_train = KMeansSMOTE().fit_resample(X_train, y_train)\n",
    "# X_train, y_train = SVMSMOTE().fit_resample(X_train, y_train)\n",
    "X_train, y_train = SMOTEENN(random_state=0).fit_resample(X_train, y_train)\n",
    "# X_train, y_train = SMOTETomek(random_state=0).fit_resample(X_train, y_train)\n",
    "print(sorted(Counter(y_train).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4471c89a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# different classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf_list = [SVC(),LinearSVC(),\n",
    "            KNeighborsClassifier(),NearestCentroid(),\n",
    "            LogisticRegression(),\n",
    "            BernoulliNB(),GaussianNB(),\n",
    "            DecisionTreeClassifier(),ExtraTreeClassifier(),                \n",
    "            RandomForestClassifier(),\n",
    "            LinearDiscriminantAnalysis(),\n",
    "            MLPClassifier(max_iter=800),\n",
    "            LogisticRegression(),\n",
    "            QuadraticDiscriminantAnalysis(),\n",
    "            RidgeClassifier(),\n",
    "            GradientBoostingClassifier(),\n",
    "            AdaBoostClassifier(),\n",
    "            BaggingClassifier()\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "110ebd7f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureY_mean         0.341163\n",
       "feature60             0.267044\n",
       "feature19             0.228706\n",
       "featureY_if           0.045305\n",
       "feature1              0.029057\n",
       "feature20_distance    0.019831\n",
       "feature16             0.014264\n",
       "featureX_mean         0.009002\n",
       "feature0              0.007826\n",
       "featureX_if           0.007704\n",
       "feature18             0.007637\n",
       "feature17             0.006314\n",
       "feature2              0.003314\n",
       "feature15             0.003025\n",
       "feature11             0.002834\n",
       "feature12             0.002609\n",
       "feature14             0.002251\n",
       "feature13             0.002112\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature importance \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "tmp = pd.Series(clf.feature_importances_,index=feature_select)\n",
    "display(tmp.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da18fe25",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# time series fluctuation analyse and binary classifier for rootcauses 1\n",
    "clf = clf_list[9]\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "\n",
    "y_predict_list = {}     \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    for sample_index,df in df_test.groupby('sample_index'):\n",
    "        df.fillna(df.interpolate(),inplace=True)\n",
    "        df.fillna(data_test.mean(),inplace=True)\n",
    "        if (np.nanmax(df['feature15']) - np.nanmin(df['feature15']) > 400):\n",
    "#             print('当前文件波动',sample_index)\n",
    "            X_test_one = df.loc[:,feature_select]\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)\n",
    "            if np.sum(y_predict_one)/len(y_predict_one) > 0.2 and sample_index < 500:\n",
    "                y_predict_list[sample_index] = 1\n",
    "            elif np.sum(y_predict_one)/len(y_predict_one) > 0.3 and sample_index > 499:\n",
    "                y_predict_list[sample_index] = 1\n",
    "            else:\n",
    "                y_predict_list[sample_index] = 0\n",
    "        else:\n",
    "            X_test_one =data_test.loc[data_test['sample_index']==sample_index,feature_select]\n",
    "            X_test_one = X_test_one.values.reshape(1,-1)\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)[0]\n",
    "            y_predict_list[sample_index] = y_predict_one\n",
    "            \n",
    "        # Too many missing values in some files\n",
    "        if sample_index in [142, 152, 153, 284, 305, 355, 361, 373, 420]:\n",
    "            y_predict_list[sample_index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e000b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = pd.Series(y_predict_list)\n",
    "y_result.to_csv('cause1_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "297755a7",
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier()\n"
     ]
    }
   ],
   "source": [
    "# time series fluctuation analyse and binary classifier\n",
    "# for rootcauses 2 \n",
    "clf = clf_list[9]\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "y_predict_list = {}     \n",
    "threshold_1 = 7\n",
    "threshold_2 = 0\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    for sample_index,df in df_test.groupby('sample_index'):\n",
    "        df.fillna(df.interpolate(),inplace=True)\n",
    "        df.fillna(data_test.mean(),inplace=True)\n",
    "        if df['feature19'].std() > threshold_1 :\n",
    "#             print('当前文件波动',sample_index)\n",
    "            X_test_one = df.loc[:,feature_select]\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)\n",
    "            if np.sum(y_predict_one) > threshold_2 :\n",
    "                y_predict_list[sample_index] = 1\n",
    "            else:\n",
    "                y_predict_list[sample_index] = 0\n",
    "        else:\n",
    "            X_test_one =data_test.loc[data_test['sample_index']==sample_index,feature_select]\n",
    "            X_test_one = X_test_one.values.reshape(1,-1)\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)[0]\n",
    "            y_predict_list[sample_index] = y_predict_one\n",
    "            \n",
    "        # Too many missing values in some files\n",
    "        if sample_index in [142, 152, 153, 284, 305, 355, 361, 373, 420]:\n",
    "            y_predict_list[sample_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d10107aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = pd.Series(y_predict_list)\n",
    "y_result.to_csv('cause2_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09894c46",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# time series fluctuation analyse and binary classifier\n",
    "# for rootcauses 3 \n",
    "clf = clf_list[9]\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)\n",
    "y_predict_list = {}     \n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "    for sample_index,df in df_test.groupby('sample_index'):\n",
    "        df.fillna(df.interpolate(),inplace=True)\n",
    "        df.fillna(data_test.mean(),inplace=True)\n",
    "        if  df['feature15'].std()>135 or df['feature13'].std() > 30000:\n",
    "#             print('当前文件波动',sample_index)\n",
    "            X_test_one = df.loc[:,feature_select]\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)\n",
    "            if np.sum(y_predict_one) > 0 :\n",
    "                y_predict_list[sample_index] = 1\n",
    "            else:\n",
    "                y_predict_list[sample_index] = 0\n",
    "        else:\n",
    "            X_test_one =data_test.loc[data_test['sample_index']==sample_index,feature_select]\n",
    "            X_test_one = X_test_one.values.reshape(1,-1)\n",
    "            X_test_one = scaler.transform(X_test_one)\n",
    "            y_predict_one = clf.predict(X_test_one)[0]\n",
    "            y_predict_list[sample_index] = y_predict_one\n",
    "            \n",
    "        # Too many missing values in some files\n",
    "        if sample_index in [142, 152, 153, 284, 305, 355, 361, 373, 420]:\n",
    "            y_predict_list[sample_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7013f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_result = pd.DataFrame(y_predict,columns=[cause_type])\n",
    "y_result.to_csv('cause3_tmp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
