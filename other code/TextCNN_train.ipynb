{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1148eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.layers import Input\n",
    "from tensorflow.python.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17355089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## file path\n",
    "data_path = r'..\\all_file'\n",
    "\n",
    "train_data_process = 'train_for_textcnn'# processed data file path\n",
    "model_name= './textcnn_with_attention_root1.h5'# model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e25e5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load and process train label\n",
    "df_train_label = pd.read_csv(os.path.join(data_path,'train_label.csv'),index_col='sample_index')\n",
    "df_train_label['Root1'] = df_train_label['root-cause(s)'].apply(lambda x : 1 if 'rootcause1' in x else 0 )\n",
    "df_train_label['Root2'] = df_train_label['root-cause(s)'].apply(lambda x : 1 if 'rootcause2' in x else 0 )\n",
    "df_train_label['Root3'] = df_train_label['root-cause(s)'].apply(lambda x : 1 if 'rootcause3' in x else 0 )\n",
    "\n",
    "y = df_train_label['Root1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8231b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files 1407\n"
     ]
    }
   ],
   "source": [
    "## load and process feature\n",
    "features = ['feature0', 'feature1', 'feature2', 'feature11', 'feature12','feature13', 'feature15',\n",
    "        'feature16', 'feature17','feature18',\n",
    "        'feature19',\n",
    "        'feature28_0', 'feature28_1', 'feature28_2', 'feature28_3',\n",
    "        'feature28_4', 'feature28_5', 'feature28_6', 'feature28_7',\n",
    "        'feature36_0', 'feature36_1', 'feature36_2', 'feature36_3',\n",
    "        'feature36_4', 'feature36_5', 'feature36_6', 'feature36_7', 'feature60',\n",
    "        'feature61_0', 'feature61_1', 'feature61_2', 'feature61_3',\n",
    "        'feature61_4', 'feature61_5', 'feature61_6', 'feature61_7','feature_edge','feature_distance','length']\n",
    "files = os.listdir(os.path.join(data_path,train_data_process))\n",
    "print('Number of files',len(files))\n",
    "files.sort(key=lambda x:int(x[:-4]))\n",
    "all_feature = []\n",
    "for filename in files:\n",
    "    df = pd.read_csv(os.path.join(data_path,train_data_process,filename),index_col = 0)\n",
    "    list_tmp = []\n",
    "    for nd in features:\n",
    "        for i in df[nd].values:\n",
    "            if type(i) == str:\n",
    "                if len(i.split(';'))> 1:\n",
    "                    i = np.array(i.split(';')).astype(float).mean()\n",
    "            list_tmp.append(i)\n",
    "    all_feature.append(list_tmp)\n",
    "all_feature = np.array(all_feature)\n",
    "all_feature[np.isnan(all_feature)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c679b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1295), (1, 1295)]\n"
     ]
    }
   ],
   "source": [
    "## data augmentation\n",
    "from imblearn.over_sampling import BorderlineSMOTE, ADASYN\n",
    "X_resampled, y_resampled = BorderlineSMOTE().fit_resample(all_feature, y)\n",
    "from collections import Counter\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e90ef635",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train and test data split \n",
    "from sklearn.model_selection import train_test_split\n",
    "def dataprocess(teature,labels):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(teature, labels, test_size=0.2) \n",
    "    return x_train,y_train, x_test, y_test\n",
    "feature_final = X_resampled.reshape(-1,len(features),30)\n",
    "X_train, y_train, X_test, y_test = dataprocess(feature_final,y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d83d7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72036175",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attention modules\n",
    "def attention_3d_block(inputs, time_steps):\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if False:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    print(a_probs.shape, inputs.shape)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80def93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 39, 30) (None, 39, 30)\n"
     ]
    }
   ],
   "source": [
    "## model\n",
    "pool_output = []\n",
    "kernel_sizes = [3, 4, 5]\n",
    "main_input = Input(shape=(X_train.shape[1],30), dtype='float64')\n",
    "O_seq = attention_3d_block(main_input, X_train.shape[1])\n",
    "for kernel_size in kernel_sizes:\n",
    "    c = Conv1D(filters=32, kernel_size=kernel_size, padding='same', strides=1)(O_seq)\n",
    "    c = BatchNormalization()(c)\n",
    "    c = Activation('relu')(c)\n",
    "    p = MaxPooling1D(pool_size=2)(c)\n",
    "    p = Flatten()(p)\n",
    "    pool_output.append(p)\n",
    "x_flatten = concatenate(pool_output)\n",
    "x_flatten = Dropout(0.4)(x_flatten)\n",
    "y = Dense(2,activation ='softmax',kernel_regularizer=regularizers.l1(0.01))(x_flatten)\n",
    "model = Model(inputs=main_input, outputs=y)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2c7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train...\n",
      "Epoch 1/300\n",
      "65/65 [==============================] - 3s 14ms/step - loss: 1.0042 - accuracy: 0.9416 - val_loss: 1.0570 - val_accuracy: 0.9923\n",
      "Epoch 2/300\n",
      " 1/65 [..............................] - ETA: 0s - loss: 0.6597 - accuracy: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py38_aiops\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 0s 7ms/step - loss: 0.4558 - accuracy: 0.9875 - val_loss: 0.6630 - val_accuracy: 0.9961\n",
      "Epoch 3/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.2275 - accuracy: 0.9932 - val_loss: 0.4444 - val_accuracy: 0.9942\n",
      "Epoch 4/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1549 - accuracy: 0.9942 - val_loss: 0.3176 - val_accuracy: 0.9961\n",
      "Epoch 5/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.1276 - accuracy: 0.9952 - val_loss: 0.2307 - val_accuracy: 0.9961\n",
      "Epoch 6/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.1122 - accuracy: 0.9957 - val_loss: 0.1515 - val_accuracy: 0.9961\n",
      "Epoch 7/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0971 - accuracy: 0.9971 - val_loss: 0.1166 - val_accuracy: 0.9961\n",
      "Epoch 8/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0854 - accuracy: 0.9981 - val_loss: 0.0938 - val_accuracy: 0.9961\n",
      "Epoch 9/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0792 - accuracy: 0.9986 - val_loss: 0.0765 - val_accuracy: 0.9981\n",
      "Epoch 10/300\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.0776 - accuracy: 0.9981 - val_loss: 0.0745 - val_accuracy: 0.9981\n",
      "Epoch 11/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0703 - accuracy: 0.9990 - val_loss: 0.0668 - val_accuracy: 0.9981\n",
      "Epoch 12/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0670 - accuracy: 0.9981 - val_loss: 0.0608 - val_accuracy: 0.9981\n",
      "Epoch 13/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.0617 - accuracy: 0.9995 - val_loss: 0.0579 - val_accuracy: 0.9981\n",
      "Epoch 14/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0596 - accuracy: 0.9990 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
      "Epoch 15/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.0588 - accuracy: 0.9986 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
      "Epoch 16/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0539 - accuracy: 0.9986 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
      "Epoch 17/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0502 - accuracy: 0.9995 - val_loss: 0.0472 - val_accuracy: 1.0000\n",
      "Epoch 18/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0500 - accuracy: 0.9986 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 19/300\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0482 - accuracy: 0.9995 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
      "Epoch 20/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.0459 - accuracy: 0.9995 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
      "Epoch 21/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.0460 - accuracy: 0.9995 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 22/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0436 - accuracy: 0.9995 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
      "Epoch 23/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0436 - accuracy: 0.9990 - val_loss: 0.0409 - val_accuracy: 1.0000\n",
      "Epoch 24/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0423 - accuracy: 0.9995 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 25/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0408 - accuracy: 0.9990 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 26/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0408 - accuracy: 0.9995 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
      "Epoch 27/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
      "Epoch 28/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
      "Epoch 29/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.0359 - val_accuracy: 1.0000\n",
      "Epoch 30/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0403 - accuracy: 0.9995 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
      "Epoch 31/300\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0367 - accuracy: 0.9995 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
      "Epoch 32/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0367 - accuracy: 0.9995 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
      "Epoch 33/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
      "Epoch 34/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
      "Epoch 35/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 36/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
      "Epoch 37/300\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.0322 - accuracy: 0.9995 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 38/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
      "Epoch 39/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
      "Epoch 40/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0331 - accuracy: 0.9995 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
      "Epoch 41/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 42/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 43/300\n",
      "65/65 [==============================] - 0s 8ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 44/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 45/300\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 46/300\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 47/300\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 48/300\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 0.0291 - accuracy: 0.9995 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 49/300\n",
      "65/65 [==============================] - 1s 9ms/step - loss: 0.0283 - accuracy: 0.9995 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 50/300\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
      "Epoch 51/300\n",
      "26/65 [===========>..................] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "## model train\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "Reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5,\n",
    "                           mode='auto', cooldown=0, min_lr=0.000001, verbose = 1)\n",
    "opt = adam_v2.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "    ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True),\n",
    "    Reduce\n",
    "]\n",
    "print('\\nTrain...')\n",
    "one_hot_labels = to_categorical(y_train, num_classes=2) \n",
    "one_hot_labels_test = to_categorical(y_test, num_classes=2) \n",
    "history = model.fit(x = X_train, y = one_hot_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=300,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, one_hot_labels_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "print(\"\\nTesting...\")\n",
    "model = load_model(model_name)\n",
    "score, accuracy = model.evaluate(X_test, one_hot_labels_test,\n",
    "                                 batch_size=64,\n",
    "                                 verbose=1)\n",
    "print(\"Test loss:  \", score)\n",
    "print(\"Test accuracy:  \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3-py38_aiops]",
   "language": "python",
   "name": "conda-env-Anaconda3-py38_aiops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
